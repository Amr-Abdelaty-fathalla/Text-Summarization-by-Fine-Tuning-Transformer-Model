# ğŸš€ Text Summarization by Fine-Tuning a Transformer Model | NLP | Hugging Face  

## ğŸ“Œ Project Overview  
This project focuses on fine-tuning a transformer-based model for text summarization using the Hugging Face ecosystem. The model is trained on a dataset of text summaries to generate concise and meaningful summaries from longer pieces of text.  

## ğŸ› ï¸ Technologies Used  
- ğŸ¤— **Hugging Face** â€“ Transformers, Datasets, and Tokenizers  
- ğŸ“Š **Weights & Biases (W&B)** â€“ Experiment tracking and visualization  

## ğŸ“– Features  
- Fine-tuning a transformer model for text summarization  
- Custom dataset preparation and preprocessing  
- Training with Weights & Biases for logging and visualization  
- Evaluation and inference for generating summaries  

## ğŸ“¦ Installation  
To set up the project, clone the repository and install the required dependencies:  
```bash  
git clone https://github.com/Amr-Abdelaty-fathalla/text-summarization-transformer.git  
cd text-summarization-transformer  
pip install -r requirements.txt  
```

## ğŸš€ Training the Model  
Run the training script with Weights & Biases integration:  
```bash  
python train.py  
```

## ğŸ“Š Monitoring with W&B  
Track your training progress by logging into Weights & Biases:  
```bash  
wandb login  
```
Then visualize metrics and logs in the W&B dashboard.

### Example Training Results from W&B  
![-----------------------------------------------------](Text_Summarization_by_Fine_Tuning_Transformer_Model/weight and baises.png) 

## ğŸ“ Inference  
To generate summaries using the fine-tuned model:  
```bash  
python inference.py --text "Your input text here"  
```

## ğŸ“œ Dataset  
The dataset used for training can be specified in the `Samsung/samsum`.

## ğŸ“Œ Evaluation  
- The model is evaluated using ROUGE scores.  


## ğŸ¤ Contributing  
Feel free to fork this repo, submit pull requests, or open issues for discussions.

